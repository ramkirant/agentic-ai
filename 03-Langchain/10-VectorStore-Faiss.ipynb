{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b4d722",
   "metadata": {},
   "source": [
    "## FAISS\n",
    "\n",
    "FAISS is a popular tool for efficient similarity search over vector embeddings. It is often used in machine learning, NLP and RAG applications. FAISS implements fast nearest neighbor search to provide a faster output. \n",
    "\n",
    "Before learning about Fash Nearest Neighbor Search, there are few more concepts that we need to be aware of. \n",
    "\n",
    "### Embedding Space\n",
    "When we generate embeddings (say using OpenAI Sentence Transformers, or CLIP), each piece of text, image or item is mapped to a vector in a high-dimensional space. (384D, 768D, 1536D). Think of it as a cloud of points floating in a space. The distance between points reflects semantic similarity (close = similar, far = different)\n",
    "\n",
    "### Vector Normalization\n",
    "Vector Normalization means, scaling down a vector such that its own length (magnitude) becomes 1. After normalization, all vectors lie on the same unit sphere in the embedding space. This ensures that no vector dominates purely because it has a larger magnitude. Some embedding models produces vectors of different lengths even for similar meanings. Normalization ensures only direction (semantic meaning) matters. Lets talk about this in detail. \n",
    "\n",
    " We all know, a vector is just a list of numbers representing some objects (like text, or an image). Example:\n",
    "\n",
    " - Vector for \"doberman\" -> [3,4]\n",
    " - Vector for \"pug\" -> [6,8]\n",
    "\n",
    "Both these vectors are dogs. But, the vector for put is double the value of doberman, which may give a impression that they are not related to each other. Vector normalization removes this difference. \n",
    "\n",
    "Vector normalization scales the vectors such that their length (magnitude) becomes 1 without changing its direction. \n",
    "\n",
    "The formula for normalizing a vector is Vn = V/Squareroot(Sum of squares of all vectors)\n",
    "\n",
    "For the above example, the vector for doberman would be (3/SqRoot(sq(3) + sq(4)),4/SqRoot(sq(3) + sq(4))) which would be [0.6,0.8]\n",
    "\n",
    "If we add the same formula for pug, the normalized vector would be [0.6, 0.8]\n",
    "\n",
    "### Nearest Neighbor Search (NNS)\n",
    "Nearest Neighbor Search can be used to retrieve documents relevant to the query. Nearest Neighbor Search looks for vectors closest to the query vector to retrieve the documents. It uses the either of the below distance metrics to retrieve the documents. \n",
    "\n",
    "1. Cosine similarity (angle between vectors)\n",
    "2. Euclidean distance (L2 norm)\n",
    "\n",
    "There are two types of Nearest Neighbor Search\n",
    "\n",
    "- **Exact Nearest Neighbor Search (ENN)**\n",
    "\n",
    "    This is a brute force search. It compares the query vector with every vector in the database. While this is 100% accurate, it is too slow for millions of embeddings. \n",
    "\n",
    "- **Approximate Nearest Neighbor Search (ANN)**\n",
    "\n",
    "    ANN uses clever data structures like trees, graphs or clustering to speed things up. It trades a tiny bit of accuracy for huge performance gains. ANN is widely used in RAG applications. \n",
    "\n",
    "\n",
    "### Distance Metrics \n",
    "Distance Metrics are various approaches that can be followed to find the distance between the vectors in Nearest Neighbor Search. The distance between the vectors determine the similarity between them. Closer the vectors, more similar they are. There are various distance metrics that are used.\n",
    "\n",
    "1. **Euclidean Distance (L2 Norm)**\n",
    "\n",
    "    - Formulae\n",
    "\n",
    "        ![alt text](euclidean-distance-formulae.jpg \"Euclidean Distance\")\n",
    "    \n",
    "    - Measures the straight line geometric distance between two vectors. \n",
    "    - Intuition: \"How apart are the points in space?\"\n",
    "    - Use case: Works well when both the magnitude and the direction of the vectors matter. (Ex: Clustering images or sensor data)\n",
    "\n",
    "2. **Manhattan Distance (L1 Norm)**\n",
    "\n",
    "    - Formulae\n",
    "\n",
    "        ![alt text](manhattan-distance-formulae.jpg \"Manhattan Distance\")\n",
    "    \n",
    "    - Measures distance as if we can only move along the grid lines. Like city blocks in Manhattan.\n",
    "    - Intuition: More robust to outliers than euclidean distance. (I doubt this)\n",
    "    - Use case: Sometimes used in sparse embeddings or table data\n",
    "\n",
    "3. **Cosine Similarity / Cosine Distance**\n",
    "\n",
    "    - Formulae\n",
    "\n",
    "        ![alt text](cosine-similarity-formulae.jpg \"Cosine Similarity\")\n",
    "\n",
    "        Cosine Similarity = 1 - sim(x,y)\n",
    "\n",
    "    - Focuses on the angle between the vectors\n",
    "    - Intuition: Two vectors pointing in the same direction are similar even though they differ in length. \n",
    "    - Use case: Very common in semantic embeddings (text embeddings, document search, RAG memory) since meaning is captured by direction and not length.\n",
    "\n",
    "4. **Dot Product / Inner Product**\n",
    "\n",
    "    - Formulae\n",
    "\n",
    "        ![alt text](dot-product-formula.jpg \"Dot Product\")\n",
    "\n",
    "    - Dot Product is similar to cosine similarity. We can consider cosine similarity to be a normalized form of dot product. \n",
    "    - Dot Product considers magnitude in the score while cosine similarity removes it. \n",
    "    - We can consider cosine similarity as pure directional closeness. Dot Product is directional closeness weighted by size. \n",
    "\n",
    "### k-means clustering\n",
    "Clustering is grouping of similar data-points together. k-means is unsupervised machine learning algorithm that partitions a dataset into **k clusters**. Here k is a predefined number of clusters that we define. In FAISS, this is called as **nlist**. \n",
    "\n",
    "Each cluster has a **centroid** and data-points that are closest to that centroid compared to other centroids. \n",
    "\n",
    "#### How k-means clustering works?\n",
    "For a given value of k, the algorithm would\n",
    "\n",
    "1. **Initialize centroids** randomly. It would pick k random points as initial cluster centers. \n",
    "2. **Assigns points to clusters**. Each data point goes to the cluster whose centroid is nearest. (using a distance metric, usually Euclidean). Can two points be equidistant to two or more centroids? While this is theoretically possible, practically, this is rare as the distance is computed as a floating point number. In theory if it happens, the algorithm will assign the data point to the first centroid it loads to the memory. \n",
    "3. **Recompute the centroids**. After all the data-points are assigned to their random centroids, The algorithm would **compute their mean (average) position** across each dimension. That average becomes the new centroid of the cluster. This is repeated for every cluster. \n",
    "\n",
    "#### When is this algorithm stopped?\n",
    "1. **Centroids don't change anymore (Convergence)**\n",
    "    If the new centroids are the same (or almost the same) as the old ones, the algorithm has converged. This means the clusters are stable.\n",
    "\n",
    "2. **Point assignments don't change**\n",
    "    Even if the centroids move a little, if every point remains assigned to the same cluster as the previous iteration, the process can stop. \n",
    "\n",
    "3. **Maximum number of iterations reached**\n",
    "    To avoid infinite loops, a maximum limit like 100 or 300 iterations is set. If the algorithm hasn't converged by then, it stops. \n",
    "\n",
    "Most implementations (like FAISS) use:\n",
    "- **Tolerance**: If the centroid movement is smaller than a small threshold (say 10-4), stop. \n",
    "- **Max iterations**: Even if it is not fully converged, stop after N iterations. \n",
    "\n",
    "### Compressing Vectors\n",
    "\n",
    "#### Product Quantization (PQ)\n",
    "In similarity search, storing and comparing raw vectors is expense. If we consider every vector dimension to take 4 bytes of memory, 1 billion vectors with 128 dimensions would take 512 GB of RAM. We need a way to compress vectors so we can still compare them approximately but with much less memory and faster distance computation. That's where Product Quantization comes in \n",
    "\n",
    "##### The idea of Product Quantization\n",
    "Instead of representing a vector by its raw float value, PQ **splits it into smaller chunks** and quantizes each chunk separately. \n",
    "\n",
    "- **Step 1: Split the vector**\n",
    "    - Suppose we have a vector of dimensions _D_ = 128\n",
    "    - Split it into _M_ sub-vectors. If we consider _M_ to be 8, each sub-vector will have _D/M_ = 16 dimensions.\n",
    "\n",
    "- **Step 2: Quantize each sub-vector**\n",
    "    - For each sub-vector space, run **k-means** to create a code-book (dictionary) of centroids. If you choose 256 centroids (k=256), then each sub-vector can be approximated by just 1 byte. (0-255 index)\n",
    "\n",
    "- **Step 3: Encode the vector**\n",
    "    - Instead of storing the raw 128 floats, we store the **index of the closest centroid for each sub-vector. For 8 sub-vector, that would be 8 bytes in total. \n",
    "\n",
    "So the whole 128-D float vector (~512 bytes) is compressed to just 8 bytes.\n",
    "\n",
    "##### Distance Computation with PQ\n",
    "How do we search efficiently if we don't have raw vectors?\n",
    "\n",
    "1. When a **query vector** comes in, we split it into sub-vectors too. \n",
    "2. For each sub-vector, compute its distance to all centroids in the code-book. \n",
    "3. For each database vector, look up its stored centroid index in each subspace and sum up the distances. \n",
    "\n",
    "This way, distance computation is super fast. Just table lookups + additions, instead of full vector distance calculations. \n",
    "\n",
    "##### Problem with Product Quantization\n",
    "Product Quantization assumes that simply slicing the vector dimensions into contiguous blocks (example, first 8 dims and then the next 8 dims) is fine. However, **the variance or correlation between dimensions** can make this division suboptimal. \n",
    "\n",
    "The dimensions of our data can be correlated. This means one or two dimensions **vary together**. For example, if we consider height and weight of a person, they are correlated. People with higher height tend to have more weight. When we group correlated dimensions in PQ, they tend to influence the centroid creation while the other dimensions may have little to no effect. \n",
    "\n",
    "The dimensions of our data can be unevenly scaled. If some dimensions have higher numerical ranges compared to others, they tend to exert more influence on the centroid compared to other dimensions. \n",
    "\n",
    "These issues are solved by **Optimized Product Quantization**\n",
    "\n",
    "#### Optimized Product Quantization (OPQ)\n",
    "Optimized Product Quantization (OPQ) rearranges the vectors before applying Product Quantization. It does this by learning an orthogonal rotation (linear transform), that arranges the vector dimensions before applying PQ. The idea is that, by rotating the space, we can distribute the variance of data more evenly across the subspaces - so that PQ can quantize it more effectively. \n",
    "\n",
    "#### Scalar Quantization (SQ)\n",
    "Scalar Quantization (SQ), instead of storing full-precision floats for each vector dimension, it approximates each component (dimension) by a discrete value - a \"quantized\" scaler. \n",
    "\n",
    "Each dimension is treated independently unlike in PQ which splits vectors into sub vectors. \n",
    "\n",
    "##### How it works?\n",
    "- **Step 1: Choose the number of bits per dimension**\n",
    "    An 8 Bit SQ can take 256 discrete values. A 4 bit SQ can take 16 discrete values.\n",
    "\n",
    "- **Step 2: Find min/max values**\n",
    "    For each dimension across all vectors, determine the minimum (Xmin) and maximum values (Xmax)\n",
    "\n",
    "- **Step 3: Map each value to a discrete level**\n",
    "    - Each dimension range (Xmin , Xmax) is divided into 2^b levels (where b is the number of bits)\n",
    "    - Each value is replaced by the index of its nearest level. \n",
    "\n",
    "    **Example**\n",
    "    - Suppose dimension values range (minimum and maximum) is [0,1]\n",
    "    - 8-bit &rarr; 256 levels: 0.0, 0.0039, 0.0078, ..., 0.996\n",
    "    - A value of 0.45 &rarr; nearest level 0.4492 &rarr; store index 115 instead of the float. \n",
    "\n",
    "- **Step 4: Store compressed vectors**\n",
    "    - Each dimension is replaced by its quantization index. \n",
    "    - 128-D vector with 8-bit SQ &rarr; 128 bytes instead of 512 bytes (128 floats x 4 bytes).\n",
    "\n",
    "#### Key differences in Quantization methods\n",
    "\n",
    "| Feature | Product Quantization (PQ) | Scalar Quantization (SQ) |\n",
    "|---------|---------------------------|--------------------------|\n",
    "| Compression Method | Vectors split into sub-vectors | Vectors approximated by a scalar value \n",
    "| Complexity | Compute intensive | Simple - No k means per sub-vector |\n",
    "| Memory Efficiency | Very high compression possible | Less efficient than PQ at same precision |\n",
    "| Accuracy | Higher accuracy with the same code size | Less precise than PQ |\n",
    "| Speed | Comparatively slower but still fast. Slow due to looks involved | Very fast |\n",
    "\n",
    "### Indexes in FAISS\n",
    "Index in FAISS decides how vectors are inserted, stored and retrieved. Every Index works with a similarity metric. (L2 distance , dot product etc)\n",
    "\n",
    "\n",
    "1. Flat (Brute Force) Index\n",
    "\n",
    "    - It stores all vectors in memory and compares query against all vectors. It supports \n",
    "        - Euclidean distance ```IndexFlatL2```\n",
    "        - Inner product ```IndexFlatIP``` (cosine similarity if the vectors are normalized)\n",
    "    - Pros: Exact search, very simple\n",
    "    - Cons: Slow for very large datasets (O(n) comparisons)\n",
    "    - Use cases: Can be used when the dataset is small\n",
    "\n",
    "2. Inverted File Index (IVF)\n",
    "\n",
    "    Inverted File Index (IVF) divides the vector space into clusters using k-means. At search time, it only probes a subset of clusters instead of all vectors. It uses the below parameters to perform the search\n",
    "\n",
    "    - nlist &rarr; number of clusters\n",
    "    - nprobe &rarr; number of clusters searched per query \n",
    "    \n",
    "    The different variants of Inverted File Index (IVF) are \n",
    "\n",
    "    - ```IVFFlat```: Stores raw vectors in each cluster.\n",
    "    - ```IVFPQ```: Uses Product Quantization to compress vectors in each space. \n",
    "    - ```IVFSQ```: Uses Scalar Quantization to compress vectors in each space. \n",
    "\n",
    "    - Pros: Much faster than brute force, scalable to billions of vectors\n",
    "    - Cons: Approximate\n",
    "    - Use case: Large datasets where speed is important. \n",
    "\n",
    "3. HSNW (Hierarchical Navigable Small World Graphs)\n",
    "\n",
    "    Before learning about HSNW, we need to understand what a graph data structure is\n",
    "\n",
    "    **Graph Data structure**\n",
    "    A graph data structure is a non liner data structure made of vertices (nodes) and edges (links). It is used to model relationships between objects. Vertices are data-containing points and edges connect the vertices to show the relationship with them. Unlike trees, graphs have no root or leaves. Nodes can be connected in anyway allowing complex, non-hierarchical relationships. \n",
    "\n",
    "    HSNW, instead of clustering the vectors, it builds a **graph structure** to enable **fast approximate nearest neighbor (ANN) search**.\n",
    "\n",
    "    It is widely used because it offers:\n",
    "\n",
    "    - Very high recall (accuracy)\n",
    "    - Low query latency\n",
    "    - Works especially well for high dimensional vectors. \n",
    "\n",
    "    **Core Idea of HNSW**\n",
    "    1. Graph-based indexing\n",
    "        - Each vector is treated as a node in a graph\n",
    "        - Edges connect each node to its nearest neighbors\n",
    "    \n",
    "    2. Small World Property\n",
    "        - Graphs are built so that most nodes can be reached in just a few steps (like \"6 degrees of separation\").\n",
    "        - This ensures search can navigate quickly\n",
    "\n",
    "    3. Hierarchical layers\n",
    "        - The graph is built in multiple layers. \n",
    "        - Top layer is sparse. It has fewer edges &rarr; allows long jumps across the dataset.\n",
    "        - Lower layers are dense. It has more edges &rarr; allows precise local refinement. \n",
    "\n",
    "    **How Search works in HNSW?**\n",
    "    1. Start at the **top layer** where there are very few nodes.\n",
    "    2. From the entry point, **greedily walk** to the neighbor closest to the query. \n",
    "    3. Move down layer by layer, always refining the nearest candidate. \n",
    "    4. At the bottom layer (densest graph), expand the search locally to get the top k-nearest neighbors. \n",
    "\n",
    "    **Exponential Decay Function**\n",
    "    1. In HNSW, the nodes are inserted randomly into various levels based on exponential decay function. \n",
    "    2. The primary property of **Exponential Decay Function** is that, it inserts very few nodes in the top layers and it inserts almost all the nodes in the bottom layer.\n",
    "    3. The topmost layer will usually contain only one node. The probability of the topmost layer to contain more than one node is near zero. \n",
    "    4. Exponential decay function has the below formula.\n",
    "            **_l=floor(-ln(u)*m)_**\n",
    "            - **_u_** is a random number between 0 and 1\n",
    "            - **_m_** is level multiplier. It controls the rate of decay. \n",
    "            - Larger  **_m_** &rarr; slower decay &rarr; more nodes appear in higher levels\n",
    "            - Smaller **_m_** &rarr; faster decay &rarr; less nodes appear in higher levels.\n",
    "            - **_floor_** is a round down function. The values are rounded down to the nearest integer. \n",
    "            - Level multiplier is derived from a user parameter **_M_** which is the maximum number of connections per node. M will typically be between 5 and 48. Given a value for **_M_**, level multiplier is calculated with the below formula.\n",
    "\n",
    "                ![alt text](level-multiplier-formulae.jpg \"Level Multiplier\")\n",
    "\n",
    "    **How are HNSW graphs created?**\n",
    "    1. Consider we have a dataset of vectors **_V = {v1, v2, v3, v4.....vn}_** in a d-dimensional space.\n",
    "    2. We also have the below parameters\n",
    "        - **_M_**: Maximum number of connections per node\n",
    "        - **_efConstruction_**: efConstruction stands for **exploration factor during construction**. When we insert a new vector into the HSNW graph, we need to \n",
    "            1. Search for its nearest neighbors among the already inserted nodes.\n",
    "            2. Connect the new nodes to some of its neighbors (up to **_M_** neighbors)\n",
    "        efConstruction controls how thoroughly we search for neighbors before deciding which ones to connect. \n",
    "    \n",
    "    **Step 1: Initialization**\n",
    "    1. Start with an empty graph\n",
    "    2. Set the first inserted vector as the entry point. There is no particular logic in choosing the vector to be first inserted. It is simply the vector which is first inserted into the index. \n",
    "    3. Assign it to a maximum layer level. This is chosen randomly usually with an exponential distribution. \n",
    "        - A layer where a vector is inserted is chosen using exponential distribution formula. \n",
    "        - The key design is that the probability of being placed in higher layers decays exponentially. \n",
    "        - This means, most of the vectors on the vector space will reside on layer 0 and fewer and fewer vectors will reside on upper layers. \n",
    "        - This is calculated using the sampling formula. It goes as below. \n",
    "            **_l=floor(-ln(u)*m)_**\n",
    "            - **_u_** is a random number between 0 and 1\n",
    "            - **_m_** is level multiplier. It controls the rate of decay. \n",
    "            - Larger  **_m_** &rarr; slower decay &rarr; more nodes appear in higher levels\n",
    "            - Smaller **_m_** &rarr; faster decay &rarr; less nodes appear in higher levels.\n",
    "            - **_floor_** is a round down function. The values are rounded down to the nearest integer.  \n",
    "    4. In the maximum level, usually there will be only one node. This is because we are using exponential decay function and the chances of having more nodes on the highest level is very minimum. The maximum level will have atleast (and at most times) one node. \n",
    "\n",
    "\n",
    "    **Step 2: For each new vector**\n",
    "    1. Decide the layer height\n",
    "        - Decide the layer height using the exponential distribution formula.\n",
    "    2. Insert the vector in the graph\n",
    "        - We insert the vector in all the layers from the layer height that is determined to all the layers below it.\n",
    "    3. Once inserted, we need to connect the new nodes with the existing nodes in the graph. This is done in two ways. \n",
    "    4. We need to connect the newly inserted node on its top most level to a node in a level above that. That way, it will become a part of the hierarchy. We do this using the **greedy search**.\n",
    "        - Start from the top level. There will only be one node in the top level. So go down to the next level. \n",
    "        - On each level, find the vector closest to the newly inserted vector using greedy search. We start with the top level node on that level, and compute the distance between our vector and all the nodes connected to the upper level node present in that level. \n",
    "        - We hop to this node and again compare the distance of all of its connected nodes to our node, find the closest one and hop to it. \n",
    "        - We do this, till we exhaust all the connected nodes and find the node closest to the newly inserted node. \n",
    "        - We drop down one level using this node and repeat the process in the lower levels. \n",
    "    5. Once we reach the maximum level where the node is inserted, we use **best first search** instead of greedy search. \n",
    "        - In best first search, instead of following one node as we did in greedy search, we keep a priority queue of all visited nodes sorted by distance to the target. \n",
    "        - We start from an entry node that is found using greedy search. \n",
    "        - We put it in an priority queue ordered by distance to the query. \n",
    "        - We pop the closest node from the PQ and explore its neighbors.\n",
    "        - For each neighbor\n",
    "            - If not visited, we compute the distance to the query \n",
    "            - Add it to the PQ. \n",
    "        - Continue until you have checked all the nodes that could be closest to the newly inserted node.\n",
    "        - Once you identify the closest node, connect it to the newly inserted node.\n",
    "        - This closest node will become the entry node to the level below to perform the best first search in that level. \n",
    "        - \n",
    "#### Flat Indexes (Exact Search)\n",
    "Flat indexes just encode the vectors into codes of a fixed size and store them in an array of ntotal * code_size bytes. At search time, all the indexed vectors are decoded sequentially and compared to the query vectors. \n",
    "\n",
    "### Fast Nearest Neighbor Search\n",
    "Nearest Neighbor Search looks for vectors closest to the query vector. In a large RAG applications, where we may have millions of document embeddings, a naive search would compute distances of the query to every single vector which would be very slow. \n",
    "\n",
    "Fast Nearest Neighbor Search uses optimized data-structures and algorithms. Fast Nearest Neighbor Search is a general term for any optimization that makes nearest neighbor retrieval quicker than brute force. The below optimization techniques are generally employed in Fast Nearest Neighbor Search\n",
    "\n",
    "1. Hardware Level optimizations\n",
    "2. Indexing Structures for Exact NNS\n",
    "3. Approximate Nearest Neighbor (ANN) Structures\n",
    "\n",
    "### FAISS\n",
    "\n",
    "FAISS stands for Facebook AI Similarity Search. It is a library for fast nearest neighbor search in high-dimensional vector spaces. FAISS provides specialized data structures and algorithms to make Nearest Neighbor Search (NNS) faster. \n",
    "\n",
    "#### Core Features of FAISS\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
