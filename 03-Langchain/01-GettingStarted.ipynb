{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d167ed",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain is a Open Source framework designed for building applications powered by LLMs. It simplifies development process by allowing the developers to chain together various components and integrate third party components with ease. \n",
    "\n",
    "### Key Features of LangChain\n",
    "\n",
    "- LangChain provides a structured way to build applications that utilize LLMs, making it easier to develop ChatBots, Virtual Assistants and other AI Driven tools.\n",
    "- The framework allows us to combine different components such as datasources, daabases and APIs to create a more complex and capable applications. \n",
    "- LangChain provides a set of tools which helps the developers experiment quickly with various configurations and integrations reducing the time needed to build and test applications.\n",
    "- It include tools for every step of the agent development lifecycle enabling the creation of reliable and context-aware agents.\n",
    "\n",
    "LangChain is most popular for creating RAG (Retrieval Augmented Generation) applications. \n",
    "\n",
    "### Retrieval Augmented Generation (RAG) Application\n",
    "Retrieval Augmented Generation (RAG) is an AI Technique that combines two things\n",
    "\n",
    "- Retrieval: Finds relevent information from large external sources like database, document collection or web\n",
    "- Generation: Uses an LLM to create a response based on both the retrieved information and also based on its own knowledge. \n",
    "\n",
    "#### Why is RAG needed\n",
    "While LLMs are powerful, they have two inherent limitations\n",
    "\n",
    "- Their knowledge is limited to the dataset on which they are trained\n",
    "- They sometimes hallucinate (make up facts)\n",
    "\n",
    "RAG helps solve this by letting the model look things up before answering from memory\n",
    "\n",
    "### How RAG works?\n",
    "\n",
    "- User asks a question\n",
    "- A search engine or a vector database would then scan the database and retrieve relevent information based on the search query. \n",
    "- A generator, based on the question and retrieved information generates a natural language response.\n",
    "\n",
    "### Different components (steps?) in RAG application using LangChain\n",
    "\n",
    "#### Data Ingestion\n",
    "\n",
    "- Load\n",
    "\n",
    "    Data is loaded from multiple datasets. Data can be loaded frin a variety of datasets including and not limited to PDF, XLS, JSON internet. \n",
    "\n",
    "- Split\n",
    "\n",
    "    The loaded data is then split into various text / document chunks. This is crucial for efficient retrieval as it allows the model to search through smaller and most relevent pieces of information rather than searching entire documents.\n",
    "\n",
    "- Embed\n",
    "\n",
    "    In the embed stage, the data is then converted to vector embeddings. They can be either static embeddings or contextual embeddings. Need to check more about this.\n",
    "\n",
    "- Store\n",
    "\n",
    "    These embeddings are then stored in a vector store database. We have multiple vector databases namely FAISS, CHROMADB, ASTRADB\n",
    "\n",
    "#### Data Retrieval\n",
    "\n",
    "- When a question is asked, relevent information from the vector database in context to the question is retrieved.\n",
    "- The question and the retrieved information together creates a prompt.\n",
    "- This prompt is then fed to the LLM to generate a natural language response.\n",
    "\n",
    "Below is a step by step breakdown of how this happens.\n",
    "\n",
    "- User asks a question\n",
    "- The question is then passed to an embedding model like OpenAI or HuggingFace to produce a Contextual Embedding.\n",
    "- This generated contextual embedding is then used to query the vector database to fetch the relevent information.\n",
    "- Using the original question, fetched vector embeddings and optional metadata (source, page number etc), a prompt is generated.\n",
    "- This prompt is then fed to an LLM which generates a natural language response.\n",
    "\n",
    "Data Retrieval in LangChain is done by a component called Retrieval Chain. Retrieval Chain is an interface which will query the vector database, fetch the relevent information, combines both the question and the fetched information into a prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
